{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import load\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy.random import randint\n",
    "from keras.optimizers import Adam\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.models import Model\n",
    "from keras.models import Input\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Conv2DTranspose\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Concatenate\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import LeakyReLU\n",
    "from matplotlib import pyplot\n",
    "from keras.initializers import RandomNormal\n",
    "from matplotlib import image\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import dlib\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat/shape_predictor_68_face_landmarks.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_encoder_block(layer_in, n_filters, batchnorm=True):\n",
    "    # weight initialization\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    # add downsampling layer\n",
    "    g = Conv2D(n_filters, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(layer_in)\n",
    "    # conditionally add batch normalization\n",
    "    if batchnorm:\n",
    "        g = BatchNormalization()(g, training=True)\n",
    "    # leaky relu activation\n",
    "    g = LeakyReLU(alpha=0.2)(g)\n",
    "    return g\n",
    "# define a decoder block\n",
    "def decoder_block(layer_in, skip_in, n_filters, dropout=True):\n",
    "    # weight initialization\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    # add upsampling layer\n",
    "    g = Conv2DTranspose(n_filters, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(layer_in)\n",
    "    # add batch normalization\n",
    "    g = BatchNormalization()(g, training=True)\n",
    "    # conditionally add dropout\n",
    "    if dropout:\n",
    "        g = Dropout(0.5)(g, training=True)\n",
    "    # merge with skip connection\n",
    "    g = Concatenate()([g, skip_in])\n",
    "    # relu activation\n",
    "    g = Activation('relu')(g)\n",
    "    return g\n",
    "def define_generator(image_shape=(256,256,3)):\n",
    "    # weight initialization\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    # image input\n",
    "    in_image = Input(shape=image_shape)\n",
    "    # encoder model\n",
    "    e1 = define_encoder_block(in_image, 64, batchnorm=False)\n",
    "    e2 = define_encoder_block(e1, 128)\n",
    "    e3 = define_encoder_block(e2, 256)\n",
    "    e4 = define_encoder_block(e3, 512)\n",
    "    e5 = define_encoder_block(e4, 512)\n",
    "    e6 = define_encoder_block(e5, 512)\n",
    "    e7 = define_encoder_block(e6, 512)\n",
    "    # bottleneck, no batch norm and relu\n",
    "    b = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(e7)\n",
    "    b = Activation('relu')(b)\n",
    "    # decoder model\n",
    "    d1 = decoder_block(b, e7, 512)\n",
    "    d2 = decoder_block(d1, e6, 512)\n",
    "    d3 = decoder_block(d2, e5, 512)\n",
    "    d4 = decoder_block(d3, e4, 512, dropout=False)\n",
    "    d5 = decoder_block(d4, e3, 256, dropout=False)\n",
    "    d6 = decoder_block(d5, e2, 128, dropout=False)\n",
    "    d7 = decoder_block(d6, e1, 64, dropout=False)\n",
    "    # output\n",
    "    g = Conv2DTranspose(3, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d7)\n",
    "    out_image = Activation('tanh')(g)\n",
    "    # define model\n",
    "    model = Model(in_image, out_image)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "##create model\n",
    "image_shape = (256,256,3)\n",
    "g_model = define_generator(image_shape)\n",
    "model = g_model\n",
    "model.load_weights('model_full2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "FACIAL_LANDMARKS_IDXS = OrderedDict([\n",
    "    (\"eye_l\", (36, 42)),\n",
    "    (\"eye_r\",(42, 48))\n",
    "])\n",
    "\n",
    "def highligther(image, shape, colors, alpha=0.75):\n",
    "    overlay = image.copy()\n",
    "    output = image.copy()\n",
    "    for (i, name) in enumerate(FACIAL_LANDMARKS_IDXS.keys()):\n",
    "        (j, k) = FACIAL_LANDMARKS_IDXS[name]\n",
    "        pts = shape[j:k]\n",
    "        hull = cv2.convexHull(pts)\n",
    "        cv2.drawContours(overlay, [hull], -1, colors[i], -1)\n",
    "        \n",
    "        hull = cv2.convexHull(pts, returnPoints = False)\n",
    "\n",
    "    cv2.addWeighted(overlay, alpha, output, 1 - alpha, 0, output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eye_color_makeup(file_name,color):\n",
    "    try:\n",
    "        img = cv2.imread(file_name)\n",
    "        faces = detector(img)\n",
    "        face = faces[0]\n",
    "        dlib_shape = predictor(img,face)\n",
    "\n",
    "        shape_2d = np.array([[p.x,p.y]for p in dlib_shape.parts()])\n",
    "\n",
    "        eye_x_l2=shape_2d[36][0]-5\n",
    "        eye_y_l2=shape_2d[37][1]-5\n",
    "        eye_x2_l2=shape_2d[39][0]+5\n",
    "        eye_y2_l2=shape_2d[40][1]+5\n",
    "\n",
    "        eye_x_r2=shape_2d[42][0]-5\n",
    "        eye_y_r2=shape_2d[43][1]-5\n",
    "        eye_x2_r2=shape_2d[45][0]+5\n",
    "        eye_y2_r2=shape_2d[46][1]+5\n",
    "        ###################################나중에 수정\n",
    "        colors = [(50, 72, 93), (45, 72, 50)]\n",
    "        ####################################\n",
    "        face_img= highligther(img,shape_2d,colors)\n",
    "\n",
    "\n",
    "        new_eye_l =face_img[eye_y_l2:eye_y2_l2,eye_x_l2:eye_x2_l2]\n",
    "        new_eye_r = face_img[eye_y_r2:eye_y2_r2,eye_x_r2:eye_x2_r2]\n",
    "\n",
    "        actual_size1 = new_eye_l.shape\n",
    "        actual_size2 = new_eye_r.shape\n",
    "        new_eye_l = cv2.resize(new_eye_l,(256,256))\n",
    "        new_eye_r = cv2.resize(new_eye_r,(256,256))\n",
    "\n",
    "        #new_eye_l = cv2.cvtColor(new_eye_l, cv2.COLOR_BGR2RGB)\n",
    "        #new_eye_r = cv2.cvtColor(new_eye_r, cv2.COLOR_BGR2RGB)\n",
    "        temp1 = np.zeros((1,256,256,3))\n",
    "        temp1[0] = new_eye_l\n",
    "        temp1 = (temp1 - 127.5) / 127.5\n",
    "\n",
    "        temp2 = np.zeros((1,256,256,3))\n",
    "        temp2[0] = new_eye_r\n",
    "        temp2 = (temp2 - 127.5) / 127.5\n",
    "\n",
    "        gen_image1 = model.predict(temp1)\n",
    "        gen_image2 = model.predict(temp2)\n",
    "        #plot_images(temp, temp2, gen_image)\n",
    "        gen_image1 = (gen_image1+1)/2.0\n",
    "        gen_image2 = (gen_image2+1)/2.0\n",
    "\n",
    "        eye_l = cv2.resize(gen_image1[0],(actual_size1[1],actual_size1[0]))\n",
    "        eye_r = cv2.resize(gen_image2[0],(actual_size2[1],actual_size2[0]))\n",
    "\n",
    "        result = img.copy()\n",
    "        result[eye_y_l2:eye_y2_l2,eye_x_l2:eye_x2_l2] = eye_l*255\n",
    "        result[eye_y_r2:eye_y2_r2,eye_x_r2:eye_x2_r2] = eye_r*255\n",
    "        return result, img\n",
    "    except IndexError:\n",
    "            print(\"face not recognized\")\n",
    "            return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "#light brown (45,72,93)\n",
    "#dark brown (70,87,96)\n",
    "#brwon [(93, 115, 127), (93, 115, 127)]\n",
    "#brwon2[(84, 80, 91), (84, 80, 91)]\n",
    "#한단계 밝게 (50,50,50)\n",
    "#두단계 밝게 (105,104,106)\n",
    "#세단계 밝게 (120,120,120)\n",
    "#dark yellow (81,103,109)\n",
    "#redish (76,76,92) [(68, 71, 115), (68, 71, 95)]\n",
    "#blue [(72, 65, 46), (72, 65, 46)]\n",
    "result, img = eye_color_makeup('sample5.jpeg',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('a',img)\n",
    "cv2.imshow('b',result)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
